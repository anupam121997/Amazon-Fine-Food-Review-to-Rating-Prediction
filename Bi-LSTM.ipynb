{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model,save_model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "#stop = stopwords.words('english')\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "addition = ['•', '!', '\"', '#', '”', '“', '$', '%', '&', \"'\", '–', '(', ')', '*','’', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '…']\n",
    "stop_words.extend(addition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(how='any',inplace=True)\n",
    "data.drop_duplicates(inplace=True, subset=['Score','Text'])\n",
    "idx = data[data[\"HelpfulnessNumerator\"]>data[\"HelpfulnessDenominator\"]].index\n",
    "data.drop(index=idx, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target(x):\n",
    "    return 2 if x>3 else 0 if x<3 else 1\n",
    "data['target'] = data['Score'].apply(create_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral = data.loc[data.target==1]\n",
    "positive = data.loc[data.target==2].sample(40000)\n",
    "negative = data.loc[data.target==0].sample(40000)\n",
    "data = pd.concat([positive, negative, neutral])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "  text = text.lower()\n",
    "  text = re.sub(r'\\@\\w+|\\#','', text)\n",
    "  text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "  text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "  result = ''.join([i for i in text if not i.isdigit()])\n",
    "  tweet_tokens = nltk.word_tokenize(result)\n",
    "  filtered_words = [w for w in tweet_tokens if not w in stop_words]\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  lemma_words = [lemmatizer.lemmatize(w, pos='a') for w in filtered_words]\n",
    "  lemma_words=\" \".join(lemma_words)\n",
    "  return lemma_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned'] = data['Text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43.166256718593424"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.cleaned.apply(lambda x: len(x.split(\" \"))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data['cleaned'].tolist()\n",
    "y = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = Tokenizer()\n",
    "token.fit_on_texts(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size  = len(token.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = token.texts_to_sequences(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 50\n",
    "X = pad_sequences(encoded_text, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('glove.6B.50d.txt', encoding='utf-8')\n",
    "\n",
    "for line in file:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    #storing the word in the variable\n",
    "    vectors = np.asarray(values[1: ])\n",
    "    #storing the vector representation of the respective word in the dictionary\n",
    "    glove_vectors[word] = vectors\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = glove_vectors.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector_matrix = np.zeros((vocab_size, 50))\n",
    "for word, index in token.word_index.items():\n",
    "    vector = glove_vectors.get(word)\n",
    "    if vector is not None:\n",
    "        word_vector_matrix[index] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_cat, random_state = 42, test_size = 0.3, stratify = y_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 50, input_length=max_length, weights = [word_vector_matrix], trainable = False),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    #tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(3,activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 50, 50)            5119850   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 50, 256)           183296    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               164352    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 5,475,949\n",
      "Trainable params: 356,099\n",
      "Non-trainable params: 5,119,850\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), \n",
    "             optimizer=tf.keras.optimizers.Adam(1e-4), \n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2402/2402 [==============================] - 253s 105ms/step - loss: 0.9821 - accuracy: 0.5459 - val_loss: 0.9496 - val_accuracy: 0.5787\n",
      "Epoch 2/100\n",
      "2402/2402 [==============================] - 268s 111ms/step - loss: 0.9432 - accuracy: 0.5871 - val_loss: 0.9360 - val_accuracy: 0.5925\n",
      "Epoch 3/100\n",
      "2402/2402 [==============================] - 245s 102ms/step - loss: 0.9286 - accuracy: 0.6021 - val_loss: 0.9212 - val_accuracy: 0.6099\n",
      "Epoch 4/100\n",
      "2402/2402 [==============================] - 245s 102ms/step - loss: 0.9167 - accuracy: 0.6159 - val_loss: 0.9130 - val_accuracy: 0.6180\n",
      "Epoch 5/100\n",
      "2402/2402 [==============================] - 249s 104ms/step - loss: 0.9092 - accuracy: 0.6241 - val_loss: 0.9098 - val_accuracy: 0.6209\n",
      "Epoch 6/100\n",
      "2402/2402 [==============================] - 246s 102ms/step - loss: 0.9014 - accuracy: 0.6334 - val_loss: 0.9029 - val_accuracy: 0.6284\n",
      "Epoch 7/100\n",
      "2402/2402 [==============================] - 242s 101ms/step - loss: 0.8958 - accuracy: 0.6386 - val_loss: 0.8997 - val_accuracy: 0.6312\n",
      "Epoch 8/100\n",
      "2402/2402 [==============================] - 234s 97ms/step - loss: 0.8901 - accuracy: 0.6456 - val_loss: 0.8981 - val_accuracy: 0.6329\n",
      "Epoch 9/100\n",
      "2402/2402 [==============================] - 233s 97ms/step - loss: 0.8867 - accuracy: 0.6504 - val_loss: 0.8934 - val_accuracy: 0.6376\n",
      "Epoch 10/100\n",
      "2402/2402 [==============================] - 232s 97ms/step - loss: 0.8810 - accuracy: 0.6564 - val_loss: 0.9088 - val_accuracy: 0.6258\n",
      "Epoch 11/100\n",
      "2402/2402 [==============================] - 231s 96ms/step - loss: 0.8771 - accuracy: 0.6605 - val_loss: 0.8929 - val_accuracy: 0.6390\n",
      "Epoch 12/100\n",
      "2402/2402 [==============================] - 233s 97ms/step - loss: 0.8732 - accuracy: 0.6647 - val_loss: 0.8929 - val_accuracy: 0.6416\n",
      "Epoch 13/100\n",
      "2402/2402 [==============================] - 232s 97ms/step - loss: 0.8692 - accuracy: 0.6688 - val_loss: 0.8886 - val_accuracy: 0.6420\n",
      "Epoch 14/100\n",
      "2402/2402 [==============================] - 231s 96ms/step - loss: 0.8656 - accuracy: 0.6727 - val_loss: 0.8868 - val_accuracy: 0.6459\n",
      "Epoch 15/100\n",
      "2402/2402 [==============================] - 232s 97ms/step - loss: 0.8621 - accuracy: 0.6787 - val_loss: 0.8848 - val_accuracy: 0.6491\n",
      "Epoch 16/100\n",
      "2402/2402 [==============================] - 245s 102ms/step - loss: 0.8577 - accuracy: 0.6826 - val_loss: 0.8866 - val_accuracy: 0.6457\n",
      "Epoch 17/100\n",
      "2402/2402 [==============================] - 250s 104ms/step - loss: 0.8542 - accuracy: 0.6866 - val_loss: 0.8860 - val_accuracy: 0.6465\n",
      "Epoch 18/100\n",
      "2402/2402 [==============================] - 240s 100ms/step - loss: 0.8509 - accuracy: 0.6911 - val_loss: 0.8866 - val_accuracy: 0.6479\n",
      "Epoch 19/100\n",
      "2402/2402 [==============================] - 232s 97ms/step - loss: 0.8489 - accuracy: 0.6921 - val_loss: 0.8823 - val_accuracy: 0.6506\n",
      "Epoch 20/100\n",
      "2402/2402 [==============================] - 244s 102ms/step - loss: 0.8437 - accuracy: 0.6988 - val_loss: 0.8888 - val_accuracy: 0.6462\n",
      "Epoch 21/100\n",
      "2402/2402 [==============================] - 237s 99ms/step - loss: 0.8409 - accuracy: 0.7023 - val_loss: 0.8875 - val_accuracy: 0.6499\n",
      "Epoch 22/100\n",
      "2402/2402 [==============================] - 260s 108ms/step - loss: 0.8366 - accuracy: 0.7075 - val_loss: 0.8878 - val_accuracy: 0.6487\n",
      "Epoch 23/100\n",
      "2402/2402 [==============================] - 246s 102ms/step - loss: 0.8324 - accuracy: 0.7117 - val_loss: 0.8861 - val_accuracy: 0.6526\n",
      "Epoch 24/100\n",
      "2402/2402 [==============================] - 233s 97ms/step - loss: 0.8294 - accuracy: 0.7154 - val_loss: 0.9002 - val_accuracy: 0.6393\n",
      "Epoch 25/100\n",
      "2402/2402 [==============================] - 231s 96ms/step - loss: 0.8259 - accuracy: 0.7192 - val_loss: 0.8864 - val_accuracy: 0.6510\n",
      "Epoch 26/100\n",
      "2402/2402 [==============================] - 228s 95ms/step - loss: 0.8234 - accuracy: 0.7216 - val_loss: 0.8877 - val_accuracy: 0.6508\n",
      "Epoch 27/100\n",
      "2402/2402 [==============================] - 234s 97ms/step - loss: 0.8196 - accuracy: 0.7263 - val_loss: 0.8882 - val_accuracy: 0.6493\n",
      "Epoch 28/100\n",
      "2402/2402 [==============================] - 232s 97ms/step - loss: 0.8156 - accuracy: 0.7310 - val_loss: 0.8885 - val_accuracy: 0.6503\n",
      "Epoch 29/100\n",
      "2402/2402 [==============================] - 224s 93ms/step - loss: 0.8127 - accuracy: 0.7340 - val_loss: 0.8875 - val_accuracy: 0.6519\n",
      "Epoch 30/100\n",
      "2402/2402 [==============================] - 226s 94ms/step - loss: 0.8079 - accuracy: 0.7396 - val_loss: 0.8898 - val_accuracy: 0.6514\n",
      "Epoch 31/100\n",
      "2402/2402 [==============================] - 222s 93ms/step - loss: 0.8059 - accuracy: 0.7411 - val_loss: 0.8918 - val_accuracy: 0.6484\n",
      "Epoch 32/100\n",
      "2402/2402 [==============================] - 224s 93ms/step - loss: 0.8043 - accuracy: 0.7424 - val_loss: 0.8905 - val_accuracy: 0.6500\n",
      "Epoch 33/100\n",
      "2402/2402 [==============================] - 247s 103ms/step - loss: 0.7986 - accuracy: 0.7486 - val_loss: 0.8930 - val_accuracy: 0.6486\n",
      "Epoch 34/100\n",
      "2402/2402 [==============================] - 251s 105ms/step - loss: 0.7965 - accuracy: 0.7512 - val_loss: 0.8976 - val_accuracy: 0.6444\n",
      "Epoch 35/100\n",
      "2402/2402 [==============================] - 235s 98ms/step - loss: 0.7929 - accuracy: 0.7553 - val_loss: 0.8965 - val_accuracy: 0.6454\n",
      "Epoch 36/100\n",
      "2402/2402 [==============================] - 229s 95ms/step - loss: 0.7910 - accuracy: 0.7570 - val_loss: 0.8933 - val_accuracy: 0.6495\n",
      "Epoch 37/100\n",
      "2402/2402 [==============================] - 235s 98ms/step - loss: 0.7855 - accuracy: 0.7634 - val_loss: 0.8927 - val_accuracy: 0.6491\n",
      "Epoch 38/100\n",
      "2402/2402 [==============================] - 236s 98ms/step - loss: 0.7842 - accuracy: 0.7641 - val_loss: 0.8993 - val_accuracy: 0.6431\n",
      "Epoch 39/100\n",
      "2402/2402 [==============================] - 236s 98ms/step - loss: 0.7810 - accuracy: 0.7678 - val_loss: 0.8935 - val_accuracy: 0.6493\n",
      "Epoch 40/100\n",
      "2402/2402 [==============================] - 252s 105ms/step - loss: 0.7784 - accuracy: 0.7704 - val_loss: 0.8985 - val_accuracy: 0.6441\n",
      "Epoch 41/100\n",
      "2402/2402 [==============================] - 257s 107ms/step - loss: 0.7748 - accuracy: 0.7743 - val_loss: 0.9009 - val_accuracy: 0.6424\n",
      "Epoch 42/100\n",
      "2402/2402 [==============================] - 247s 103ms/step - loss: 0.7737 - accuracy: 0.7759 - val_loss: 0.8945 - val_accuracy: 0.6501\n",
      "Epoch 43/100\n",
      "2402/2402 [==============================] - 240s 100ms/step - loss: 0.7702 - accuracy: 0.7792 - val_loss: 0.8947 - val_accuracy: 0.6489\n",
      "Epoch 44/100\n",
      "2402/2402 [==============================] - 242s 101ms/step - loss: 0.7697 - accuracy: 0.7798 - val_loss: 0.9016 - val_accuracy: 0.6413\n",
      "Epoch 45/100\n",
      "2402/2402 [==============================] - 259s 108ms/step - loss: 0.7664 - accuracy: 0.7833 - val_loss: 0.8985 - val_accuracy: 0.6451\n",
      "Epoch 46/100\n",
      "2402/2402 [==============================] - 257s 107ms/step - loss: 0.7636 - accuracy: 0.7862 - val_loss: 0.8962 - val_accuracy: 0.6487\n",
      "Epoch 47/100\n",
      "2402/2402 [==============================] - 259s 108ms/step - loss: 0.7613 - accuracy: 0.7882 - val_loss: 0.8994 - val_accuracy: 0.6441\n",
      "Epoch 48/100\n",
      "2402/2402 [==============================] - 244s 102ms/step - loss: 0.7598 - accuracy: 0.7901 - val_loss: 0.8971 - val_accuracy: 0.6469\n",
      "Epoch 49/100\n",
      "2402/2402 [==============================] - 244s 101ms/step - loss: 0.7586 - accuracy: 0.7908 - val_loss: 0.8958 - val_accuracy: 0.6481\n",
      "Epoch 50/100\n",
      "2402/2402 [==============================] - 241s 100ms/step - loss: 0.7552 - accuracy: 0.7945 - val_loss: 0.8977 - val_accuracy: 0.6477\n",
      "Epoch 51/100\n",
      "2402/2402 [==============================] - 241s 100ms/step - loss: 0.7535 - accuracy: 0.7964 - val_loss: 0.8994 - val_accuracy: 0.6454- ETA: 6s - los - E - ETA\n",
      "Epoch 52/100\n",
      "2402/2402 [==============================] - 244s 101ms/step - loss: 0.7501 - accuracy: 0.7999 - val_loss: 0.8968 - val_accuracy: 0.6482\n",
      "Epoch 53/100\n",
      "2402/2402 [==============================] - 243s 101ms/step - loss: 0.7482 - accuracy: 0.8024 - val_loss: 0.8958 - val_accuracy: 0.6491\n",
      "Epoch 54/100\n",
      "2402/2402 [==============================] - 246s 102ms/step - loss: 0.7465 - accuracy: 0.8043 - val_loss: 0.9012 - val_accuracy: 0.6436\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2402/2402 [==============================] - 233s 97ms/step - loss: 0.7444 - accuracy: 0.8057 - val_loss: 0.8974 - val_accuracy: 0.6473\n",
      "Epoch 56/100\n",
      "2402/2402 [==============================] - 231s 96ms/step - loss: 0.7423 - accuracy: 0.8077 - val_loss: 0.8992 - val_accuracy: 0.6456\n",
      "Epoch 57/100\n",
      "2402/2402 [==============================] - 234s 98ms/step - loss: 0.7402 - accuracy: 0.8099 - val_loss: 0.8998 - val_accuracy: 0.6452\n",
      "Epoch 58/100\n",
      "2402/2402 [==============================] - 231s 96ms/step - loss: 0.7386 - accuracy: 0.8115 - val_loss: 0.9023 - val_accuracy: 0.6427\n",
      "Epoch 59/100\n",
      "2402/2402 [==============================] - 233s 97ms/step - loss: 0.7393 - accuracy: 0.8106 - val_loss: 0.8980 - val_accuracy: 0.6480\n",
      "Epoch 60/100\n",
      "2402/2402 [==============================] - 232s 97ms/step - loss: 0.7382 - accuracy: 0.8119 - val_loss: 0.8998 - val_accuracy: 0.6453\n",
      "Epoch 61/100\n",
      "2402/2402 [==============================] - 234s 98ms/step - loss: 0.7359 - accuracy: 0.8142 - val_loss: 0.8959 - val_accuracy: 0.6506\n",
      "Epoch 62/100\n",
      "2402/2402 [==============================] - 234s 97ms/step - loss: 0.7347 - accuracy: 0.8157 - val_loss: 0.9037 - val_accuracy: 0.6419\n",
      "Epoch 63/100\n",
      "2402/2402 [==============================] - 225s 94ms/step - loss: 0.7319 - accuracy: 0.8187 - val_loss: 0.9027 - val_accuracy: 0.6432\n",
      "Epoch 64/100\n",
      "2402/2402 [==============================] - 224s 93ms/step - loss: 0.7305 - accuracy: 0.8200 - val_loss: 0.9042 - val_accuracy: 0.6415\n",
      "Epoch 65/100\n",
      "2402/2402 [==============================] - 227s 94ms/step - loss: 0.7295 - accuracy: 0.8210 - val_loss: 0.8972 - val_accuracy: 0.6490\n",
      "Epoch 66/100\n",
      "2402/2402 [==============================] - 222s 93ms/step - loss: 0.7286 - accuracy: 0.8219 - val_loss: 0.8991 - val_accuracy: 0.6476\n",
      "Epoch 67/100\n",
      "2402/2402 [==============================] - 226s 94ms/step - loss: 0.7253 - accuracy: 0.8252 - val_loss: 0.9011 - val_accuracy: 0.6441\n",
      "Epoch 68/100\n",
      "2402/2402 [==============================] - 224s 93ms/step - loss: 0.7261 - accuracy: 0.8243 - val_loss: 0.9011 - val_accuracy: 0.6456\n",
      "Epoch 69/100\n",
      "2402/2402 [==============================] - 222s 92ms/step - loss: 0.7255 - accuracy: 0.8249 - val_loss: 0.9023 - val_accuracy: 0.6446\n",
      "Epoch 70/100\n",
      "2402/2402 [==============================] - 227s 94ms/step - loss: 0.7222 - accuracy: 0.8283 - val_loss: 0.9006 - val_accuracy: 0.6455\n",
      "Epoch 71/100\n",
      "2402/2402 [==============================] - 224s 93ms/step - loss: 0.7205 - accuracy: 0.8303 - val_loss: 0.8999 - val_accuracy: 0.6462\n",
      "Epoch 72/100\n",
      "2402/2402 [==============================] - 225s 94ms/step - loss: 0.7234 - accuracy: 0.8272 - val_loss: 0.8999 - val_accuracy: 0.6464\n",
      "Epoch 73/100\n",
      "2402/2402 [==============================] - 226s 94ms/step - loss: 0.7215 - accuracy: 0.8291 - val_loss: 0.8989 - val_accuracy: 0.6476\n",
      "Epoch 74/100\n",
      "2402/2402 [==============================] - 222s 92ms/step - loss: 0.7185 - accuracy: 0.8322 - val_loss: 0.9005 - val_accuracy: 0.6462\n",
      "Epoch 75/100\n",
      "2402/2402 [==============================] - 225s 94ms/step - loss: 0.7178 - accuracy: 0.8330 - val_loss: 0.9000 - val_accuracy: 0.6464\n",
      "Epoch 76/100\n",
      "2402/2402 [==============================] - 224s 93ms/step - loss: 0.7181 - accuracy: 0.8324 - val_loss: 0.8984 - val_accuracy: 0.6480\n",
      "Epoch 77/100\n",
      "2402/2402 [==============================] - 222s 92ms/step - loss: 0.7171 - accuracy: 0.8337 - val_loss: 0.9020 - val_accuracy: 0.6445\n",
      "Epoch 78/100\n",
      "2402/2402 [==============================] - 227s 94ms/step - loss: 0.7151 - accuracy: 0.8355 - val_loss: 0.9064 - val_accuracy: 0.6399\n",
      "Epoch 79/100\n",
      "2402/2402 [==============================] - 218s 91ms/step - loss: 0.7131 - accuracy: 0.8379 - val_loss: 0.9012 - val_accuracy: 0.6451\n",
      "Epoch 80/100\n",
      "2402/2402 [==============================] - 216s 90ms/step - loss: 0.7127 - accuracy: 0.8379 - val_loss: 0.8992 - val_accuracy: 0.6477\n",
      "Epoch 81/100\n",
      "2402/2402 [==============================] - 220s 92ms/step - loss: 0.7156 - accuracy: 0.8353 - val_loss: 0.9009 - val_accuracy: 0.6452\n",
      "Epoch 82/100\n",
      "2402/2402 [==============================] - 215s 90ms/step - loss: 0.7118 - accuracy: 0.8390 - val_loss: 0.9017 - val_accuracy: 0.6451\n",
      "Epoch 83/100\n",
      "2402/2402 [==============================] - 217s 90ms/step - loss: 0.7142 - accuracy: 0.8365 - val_loss: 0.9034 - val_accuracy: 0.6435\n",
      "Epoch 84/100\n",
      "2402/2402 [==============================] - 217s 90ms/step - loss: 0.7079 - accuracy: 0.8427 - val_loss: 0.9018 - val_accuracy: 0.6444\n",
      "Epoch 85/100\n",
      "2402/2402 [==============================] - 216s 90ms/step - loss: 0.7103 - accuracy: 0.8405 - val_loss: 0.9017 - val_accuracy: 0.6455\n",
      "Epoch 86/100\n",
      "2402/2402 [==============================] - 219s 91ms/step - loss: 0.7083 - accuracy: 0.8424 - val_loss: 0.9075 - val_accuracy: 0.6395\n",
      "Epoch 87/100\n",
      "2402/2402 [==============================] - 215s 90ms/step - loss: 0.7076 - accuracy: 0.8433 - val_loss: 0.9016 - val_accuracy: 0.6455\n",
      "Epoch 88/100\n",
      "2402/2402 [==============================] - 216s 90ms/step - loss: 0.7058 - accuracy: 0.8449 - val_loss: 0.8998 - val_accuracy: 0.6474\n",
      "Epoch 89/100\n",
      "2402/2402 [==============================] - 217s 90ms/step - loss: 0.7048 - accuracy: 0.8461 - val_loss: 0.9038 - val_accuracy: 0.6429\n",
      "Epoch 90/100\n",
      "2402/2402 [==============================] - 215s 89ms/step - loss: 0.7054 - accuracy: 0.8455 - val_loss: 0.9036 - val_accuracy: 0.6429\n",
      "Epoch 91/100\n",
      "2402/2402 [==============================] - 215s 89ms/step - loss: 0.7032 - accuracy: 0.8480 - val_loss: 0.9038 - val_accuracy: 0.6423\n",
      "Epoch 92/100\n",
      "2402/2402 [==============================] - 229s 95ms/step - loss: 0.7045 - accuracy: 0.8462 - val_loss: 0.9021 - val_accuracy: 0.6445\n",
      "Epoch 93/100\n",
      "2402/2402 [==============================] - 216s 90ms/step - loss: 0.7032 - accuracy: 0.8478 - val_loss: 0.8998 - val_accuracy: 0.6477\n",
      "Epoch 94/100\n",
      "2402/2402 [==============================] - 215s 89ms/step - loss: 0.7012 - accuracy: 0.8496 - val_loss: 0.9088 - val_accuracy: 0.6378\n",
      "Epoch 95/100\n",
      "2402/2402 [==============================] - 216s 90ms/step - loss: 0.7010 - accuracy: 0.8498 - val_loss: 0.9009 - val_accuracy: 0.6466\n",
      "Epoch 96/100\n",
      "2402/2402 [==============================] - 215s 89ms/step - loss: 0.7021 - accuracy: 0.8485 - val_loss: 0.9016 - val_accuracy: 0.6458\n",
      "Epoch 97/100\n",
      "2402/2402 [==============================] - 218s 91ms/step - loss: 0.7009 - accuracy: 0.8497 - val_loss: 0.9013 - val_accuracy: 0.6464\n",
      "Epoch 98/100\n",
      "2402/2402 [==============================] - 215s 90ms/step - loss: 0.6985 - accuracy: 0.8527 - val_loss: 0.9054 - val_accuracy: 0.6421\n",
      "Epoch 99/100\n",
      "2402/2402 [==============================] - 215s 89ms/step - loss: 0.6973 - accuracy: 0.8537 - val_loss: 0.9001 - val_accuracy: 0.6475\n",
      "Epoch 100/100\n",
      "2402/2402 [==============================] - 219s 91ms/step - loss: 0.7001 - accuracy: 0.8505 - val_loss: 0.9009 - val_accuracy: 0.6466\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x = X_train, y = y_train, epochs = 100, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1030/1030 [==============================] - 26s 26ms/step - loss: 0.9009 - accuracy: 0.6466\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6465640068054199"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model.evaluate(x = X_test, y = y_test)\n",
    "y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
