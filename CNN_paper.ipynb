{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pickle\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "addition = ['•', '!', '\"', '#', '”', '“', '$', '%', '&', \"'\", '–', '(', ')', '*','’', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '…']\n",
    "stop_words.extend(addition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(how='any',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(inplace=True, subset=['Score','Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = data[data[\"HelpfulnessNumerator\"]>data[\"HelpfulnessDenominator\"]].index\n",
    "data.drop(index=idx, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    250737\n",
       "4     56073\n",
       "1     36277\n",
       "3     29770\n",
       "2     20802\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109770, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_1 = data.loc[data.Score==1].sample(25000)\n",
    "class_2 = data.loc[data.Score==2].sample(15000)\n",
    "class_3 = data.loc[data.Score==3].sample(29770)\n",
    "class_4 = data.loc[data.Score==4].sample(15000)\n",
    "class_5 = data.loc[data.Score==5].sample(25000)\n",
    "data = pd.concat([class_1,class_2,class_3,class_4,class_5])\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target(x):\n",
    "    if x > 3:\n",
    "        return 2\n",
    "    elif x < 3:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "data['target'] = data['Score'].apply(create_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_read = open(\"glove_dictionary_100d\", \"rb\")\n",
    "glove_model = pickle.load(file_to_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tweet_tokens = nltk.word_tokenize(text)\n",
    "    #filtered_words = [w for w in tweet_tokens if not w in stop_words]\n",
    "    #sent =\" \".join(filtered_words)\n",
    "    sent =\" \".join(tweet_tokens)\n",
    "    return sent\n",
    "\n",
    "data['Text'] = data['Text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Text.apply(lambda x: len(x.split(\" \"))).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train = data['Text'].values\n",
    "y_train = data['target'].values\n",
    "\n",
    "y_cat = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105554\n"
     ]
    }
   ],
   "source": [
    "X_t=[]\n",
    "cnt=0\n",
    "t_lst=[0.001 for i in range(100)]\n",
    "for s_i in range(0,109770):\n",
    "  my_lst=[]\n",
    "  sent=sentences_train[s_i]\n",
    "  word_lst=word_tokenize(sent)\n",
    "  len_w_l=len(word_lst)\n",
    "  for i in range(60):\n",
    "    try:\n",
    "      if(i<len_w_l):\n",
    "        my_lst.append(glove_model[word_lst[i]])\n",
    "      else:\n",
    "        my_lst.append(t_lst)  \n",
    "    except:\n",
    "      cnt+=1\n",
    "      my_lst.append(t_lst)\n",
    "  X_t.append(my_lst)\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape =  (109770, 60, 100)\n"
     ]
    }
   ],
   "source": [
    "X_train=np.array(X_t)\n",
    "print(\"X_train shape = \",X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109770,)\n",
      "y_train_2 shape =  (109770, 1, 3)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    geeky_file = open('X_train_final', 'wb')\n",
    "    pickle.dump(X_train, geeky_file)\n",
    "    geeky_file.close()\n",
    "  \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"Something went wrong\")\n",
    "    \n",
    "print(y_train.shape)\n",
    "y_train_2=np.reshape(y_cat,(109770,1,3))\n",
    "\n",
    "try:\n",
    "    geeky_file = open('y_train_final', 'wb')\n",
    "    pickle.dump(y_train_2, geeky_file)\n",
    "    geeky_file.close()\n",
    "  \n",
    "except:\n",
    "    print(\"Something went wrong\")\n",
    "    \n",
    "print(\"y_train_2 shape = \",y_train_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file_to_read = open(\"X_train_final\", \"rb\")\n",
    "X = pickle.load(file_to_read)\n",
    "file_to_read = open(\"y_train_final\", \"rb\")\n",
    "y = pickle.load(file_to_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:76839]\n",
    "X_test = X[76839:]\n",
    "y_train = y[:76839]\n",
    "y_test = y[76839:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2402/2402 [==============================] - 40s 17ms/step - loss: 1.0163 - accuracy: 0.5865\n",
      "Epoch 2/10\n",
      "2402/2402 [==============================] - 38s 16ms/step - loss: 0.7798 - accuracy: 0.6492\n",
      "Epoch 3/10\n",
      "2402/2402 [==============================] - 37s 15ms/step - loss: 0.7336 - accuracy: 0.6741\n",
      "Epoch 4/10\n",
      "2402/2402 [==============================] - 40s 17ms/step - loss: 0.6868 - accuracy: 0.6992\n",
      "Epoch 5/10\n",
      "2402/2402 [==============================] - 43s 18ms/step - loss: 0.6482 - accuracy: 0.72000s - loss: 0.6483 - accura\n",
      "Epoch 6/10\n",
      "2402/2402 [==============================] - 38s 16ms/step - loss: 0.6089 - accuracy: 0.7405\n",
      "Epoch 7/10\n",
      "2402/2402 [==============================] - 36s 15ms/step - loss: 0.5743 - accuracy: 0.7551\n",
      "Epoch 8/10\n",
      "2402/2402 [==============================] - 35s 15ms/step - loss: 0.5410 - accuracy: 0.7722\n",
      "Epoch 9/10\n",
      "2402/2402 [==============================] - 32s 13ms/step - loss: 0.5130 - accuracy: 0.7847\n",
      "Epoch 10/10\n",
      "2402/2402 [==============================] - 32s 13ms/step - loss: 0.4908 - accuracy: 0.7949\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a538c94c40>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "title_input = tf.keras.Input(\n",
    "    shape=(60,100), name=\"title\"\n",
    ")\n",
    "y1=tf.keras.layers.Conv1D(32,2,activation='relu',input_shape=(60,100))(title_input)\n",
    "y2=tf.keras.layers.Conv1D(32,2,activation='relu',input_shape=(60,100))(title_input)\n",
    "y3=tf.keras.layers.Conv1D(32,3,activation='relu',input_shape=(60,100))(title_input)\n",
    "y4=tf.keras.layers.Conv1D(32,3,activation='relu',input_shape=(60,100))(title_input)\n",
    "y5=tf.keras.layers.Conv1D(32,4,activation='relu',input_shape=(60,100))(title_input)\n",
    "y6=tf.keras.layers.Conv1D(32,4,activation='relu',input_shape=(60,100))(title_input)\n",
    "\n",
    "m1=tf.keras.layers.MaxPooling1D(pool_size=59,strides=1)(y1)\n",
    "m2=tf.keras.layers.MaxPooling1D(pool_size=59,strides=1)(y2)\n",
    "m3=tf.keras.layers.MaxPooling1D(pool_size=58,strides=1)(y3)\n",
    "m4=tf.keras.layers.MaxPooling1D(pool_size=58,strides=1)(y4)\n",
    "m5=tf.keras.layers.MaxPooling1D(pool_size=57,strides=1)(y5)\n",
    "m6=tf.keras.layers.MaxPooling1D(pool_size=57,strides=1)(y6)\n",
    "\n",
    "concanate = tf.keras.layers.concatenate([m1,m2,m3,m4,m5,m6])\n",
    "\n",
    "den_lay1 = tf.keras.layers.Dense(256, name=\"MLP1\")(concanate)\n",
    "\n",
    "den_lay2 = tf.keras.layers.Dense(128, name=\"MLP2\")(den_lay1)\n",
    "\n",
    "den_lay3 = tf.keras.layers.Dense(64, name=\"MLP3\")(den_lay2)\n",
    "\n",
    "den_lay4 = tf.keras.layers.Dense(32, name=\"MLP4\")(den_lay3)\n",
    "\n",
    "den_lay5 = tf.keras.layers.Dense(16, name=\"MLP5\")(den_lay4)\n",
    "\n",
    "op_lay = tf.keras.layers.Dense(3, activation='softmax',name=\"priority\")(den_lay5)\n",
    "\n",
    "model = tf.keras.Model(\n",
    "    inputs=[title_input],\n",
    "    outputs=[op_lay],\n",
    ")\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train,y_train,epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1030/1030 [==============================] - 5s 5ms/step - loss: 0.9529 - accuracy: 0.6331\n",
      "0.6331419944763184\n"
     ]
    }
   ],
   "source": [
    "score=model.evaluate(X_test,y_test)\n",
    "print(score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
